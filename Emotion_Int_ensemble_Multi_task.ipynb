{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TcL6nkbir2i",
        "outputId": "03b119fc-e34c-4aae-eb72-09441793b4c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, LinearRegression\n",
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt5Lwl6Ai4cM"
      },
      "outputs": [],
      "source": [
        "anger_training_set = []\n",
        "fear_training_set = []\n",
        "sadness_training_set = []\n",
        "joy_training_set = []\n",
        "\n",
        "anger_test_set = []\n",
        "fear_test_set = []\n",
        "sadness_test_set = []\n",
        "joy_test_set = []\n",
        "stemmer = LancasterStemmer()\n",
        "all_words=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41E8GOgDi80K"
      },
      "outputs": [],
      "source": [
        "def load_training_data(sentiment):\n",
        "    data = open(\"/content/\"+sentiment+\"_training_set.txt\",encoding=\"utf8\")\n",
        "    if sentiment == \"anger\":\n",
        "        threshold = 0.5\n",
        "    elif sentiment == \"fear\":\n",
        "        threshold = 0.6\n",
        "    elif sentiment == \"sadness\":\n",
        "        threshold = 0.5\n",
        "    elif sentiment == \"joy\":\n",
        "        threshold = 0.5\n",
        "    else:\n",
        "        pass\n",
        "    return data,threshold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGi_vGRKi_pY"
      },
      "outputs": [],
      "source": [
        "def load_test_data(sentiment):\n",
        "    data = open(\"/content/\"+sentiment+\"_test_set.txt\",encoding=\"utf8\")\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPo9FfMOjS5C"
      },
      "outputs": [],
      "source": [
        "def clean_data(training_data,threshold):\n",
        "    training_set = []\n",
        "    for line in training_data:\n",
        "        line = line.strip().lower()\n",
        "        if line.split()[-1] == \"none\":\n",
        "            line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
        "            punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
        "            result = line.translate(punct)\n",
        "            tokened_sentence = nltk.word_tokenize(result)\n",
        "            sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
        "            label = tokened_sentence[-2]\n",
        "            training_set.append((sentence,label))\n",
        "        else:\n",
        "            intensity = float(line.split()[-1])\n",
        "            if (intensity>=threshold):\n",
        "                line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
        "                punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
        "                result = line.translate(punct)\n",
        "                tokened_sentence = nltk.word_tokenize(result)\n",
        "                sentence = tokened_sentence[0:len(tokened_sentence)-1]\n",
        "                label = tokened_sentence[-1]\n",
        "                training_set.append((sentence,label))\n",
        "    return training_set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE4jCGK3jYh7"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(all_data):\n",
        "    training_set = []\n",
        "    all_words = []\n",
        "    for each_list in all_data:\n",
        "        for words in each_list[0]:\n",
        "            word = stemmer.stem(words)\n",
        "            all_words.append(word)\n",
        "    all_words = list(set(all_words))\n",
        "\n",
        "    for each_sentence in all_data:\n",
        "        bag = [0]*len(all_words)\n",
        "        training_set.append(encode_sentence(all_words,each_sentence[0],bag))\n",
        "    return training_set,all_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwG-M6UFjb5t"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(all_words,sentence, bag):\n",
        "    for s in sentence:\n",
        "        stemmed_word = stemmer.stem(s)\n",
        "        for i,word in enumerate(all_words):\n",
        "            if stemmed_word == word:\n",
        "                bag[i] = 1\n",
        "    return bag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tS6h6iQjp0T"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    bag = []\n",
        "    all_data = []\n",
        "    all_test_data = []\n",
        "    labels = []\n",
        "    classes = []\n",
        "    labels = []\n",
        "    test_labels = []\n",
        "    words=[]\n",
        "    test_words = []\n",
        "\n",
        "    \n",
        "    anger_training_data,threshold = load_training_data(\"anger\")\n",
        "    anger_training_set = clean_data(anger_training_data,threshold)\n",
        "    print(anger_training_set[0])\n",
        "\n",
        "    fear_training_data,threshold = load_training_data(\"fear\")\n",
        "    fear_training_set = clean_data(fear_training_data,threshold)\n",
        "\n",
        "    sadness_training_data,threshold = load_training_data(\"sadness\")\n",
        "    sadness_training_set = clean_data(sadness_training_data,threshold)\n",
        "\n",
        "    joy_training_data,threshold = load_training_data(\"joy\")\n",
        "    joy_training_set = clean_data(joy_training_data,threshold)\n",
        "\n",
        "\n",
        "    anger_test_data = load_test_data(\"anger\")\n",
        "    anger_test_set = clean_data(anger_test_data,threshold)\n",
        "    print(anger_test_set[0])\n",
        "    print(len(anger_test_set))\n",
        "\n",
        "    fear_test_data = load_test_data(\"fear\")\n",
        "    fear_test_set = clean_data(fear_test_data,threshold)\n",
        "    print(fear_test_set[0])\n",
        "    print(len(fear_test_set))\n",
        "\n",
        "    sadness_test_data = load_test_data(\"sadness\")\n",
        "    sadness_test_set = clean_data(sadness_test_data,threshold)\n",
        "    print(sadness_test_set[0])\n",
        "    print(len(sadness_test_set))\n",
        "\n",
        "    joy_test_data = load_test_data(\"joy\")\n",
        "    joy_test_set = clean_data(joy_test_data,threshold)\n",
        "    print(joy_test_set[0])\n",
        "    print(len(joy_test_set))\n",
        "\n",
        "\n",
        "    all_data.extend(anger_training_set)\n",
        "    all_data.extend(fear_training_set)\n",
        "    all_data.extend(sadness_training_set)\n",
        "    all_data.extend(joy_training_set)\n",
        "\n",
        "    all_data.extend(anger_test_set)\n",
        "    all_data.extend(fear_test_set)\n",
        "    all_data.extend(sadness_test_set)\n",
        "    all_data.extend(joy_test_set)\n",
        "\n",
        "\n",
        "\n",
        "    for i,j in all_data:\n",
        "        if j == \"anger\":\n",
        "            labels.append([1,0,0,0])\n",
        "        elif j == \"fear\":\n",
        "            labels.append([0,1,0,0])\n",
        "        elif j == \"sadness\":\n",
        "            labels.append([0,0,1,0])\n",
        "        elif j == \"joy\":\n",
        "            labels.append([0,0,0,1])\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    print(len(labels))\n",
        "    print(len(test_labels))\n",
        "    classes = [\"anger\",\"fear\",\"sadness\",\"joy\"]\n",
        "    print(classes)\n",
        "    np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "\n",
        "    training_set,words = bag_of_words(all_data)\n",
        "\n",
        "    dataset = np.array(training_set)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "\n",
        "    shuffling_function = np.random.permutation(dataset.shape[0])\n",
        "    shuffled_dataset, shuffled_labels = np.zeros((dataset.shape)),np.zeros((dataset.shape))\n",
        "    shuffled_dataset,shuffled_labels = dataset[shuffling_function],labels[shuffling_function]\n",
        "\n",
        "\n",
        "    split = int(len(shuffled_dataset)*0.8)\n",
        "    training_data = shuffled_dataset[:split]\n",
        "    training_labels = shuffled_labels[:split]\n",
        "    test_data = shuffled_dataset[split:]\n",
        "    test_labels = shuffled_labels[split:]\n",
        "    print(training_data.shape)\n",
        "    print(training_labels.shape)\n",
        "    print(test_data.shape)\n",
        "    print(test_labels.shape)\n",
        "\n",
        "\n",
        "    Train_model(training_data,training_labels,words,classes)\n",
        "    Test_model(test_data,test_labels,words,classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFTivuPvjuEi"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return (1/(1+np.exp(-z)))\n",
        "\n",
        "def relu(z):\n",
        "    A = np.array(z,copy=True)\n",
        "    A[z<0]=0\n",
        "    assert A.shape == z.shape\n",
        "    return A\n",
        "\n",
        "def softmax(x):\n",
        "    num = np.exp(x-np.amax(x,axis=0,keepdims=True))\n",
        "    return num/np.sum(num,axis=0,keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edx9Raqqj4z1"
      },
      "outputs": [],
      "source": [
        "\n",
        "def forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2):\n",
        "    Z1 = np.dot(W1,X)+b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(W2,A1)+b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1,A1,Z2,A2\n",
        "\n",
        "def relu_backward(da,dz):\n",
        "    da1 = np.array(da,copy=True)\n",
        "    da1[dz<0]=0\n",
        "    assert da1.shape == dz.shape\n",
        "    return da1\n",
        "\n",
        "def linear_backward(dz,a,m,w,b):\n",
        "    dw = (1/m)*np.dot(dz,a.T)\n",
        "    db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
        "    da = np.dot(w.T,dz)\n",
        "    assert (dw.shape==w.shape)\n",
        "    assert (da.shape==a.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    return da,dw,db\n",
        "\n",
        "\n",
        "def calculate_loss(Y,Yhat,m):\n",
        "    loss = (-1/m)*np.sum(np.multiply(Y,np.log(Yhat)))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m):\n",
        "    dZ2 = A2-Y\n",
        "    da1,dw2,db2 = linear_backward(dZ2,A1,m,W2,b2)\n",
        "    dZ1 = relu_backward(da1,Z1)\n",
        "    da0,dw1,db1 = linear_backward(dZ1,X,m,W1,b1)\n",
        "    W2 = W2 - learning_rate * dw2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    W1 = W1 - learning_rate * dw1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    return W1,b1,W2,b2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZTR6PkEj8la"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def Test_model(test_data, test_labels,words,classes):\n",
        "    all_losses = []\n",
        "    learning_rate = 0.1\n",
        "    iterations = 50\n",
        "    np.random.seed(1)\n",
        "    X = test_data.T\n",
        "    print(\" Shape of X is \", X.shape)\n",
        "    Y = test_labels.T\n",
        "    print(\" Shape of Y is \", Y.shape)\n",
        "\n",
        "    m = X.shape[1]\n",
        "    print(\" Shape of m is \", m)\n",
        "   \n",
        "    n_h = 100\n",
        "  \n",
        "    n_x = X.shape[0]\n",
        "\n",
        "    n_y = 4\n",
        "\n",
        "    weights_file = 'weights.json'\n",
        "    with open(weights_file) as data_file:\n",
        "        weights = json.load(data_file)\n",
        "        W1 = np.asarray(weights['weight1'])\n",
        "        W2 = np.asarray(weights['weight2'])\n",
        "        b1 = np.asarray(weights['bias1'])\n",
        "        b2 = np.asarray(weights['bias2'])\n",
        "\n",
        "    print(\"################### TEST MODEL STATISTICS ######################\")\n",
        "    for i in range(1):\n",
        "        \n",
        "        l0 = X\n",
        "       \n",
        "        l1 = relu(np.dot(W1,l0)+b1)\n",
        "        # output layer\n",
        "        l2 = softmax(np.dot(W2,l1)+b2)\n",
        "        predictions = np.argmax(l2, axis=0)\n",
        "        labels = np.argmax(Y, axis=0)\n",
        "        print(classification_report(predictions,labels))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJKH61uDiQ8W"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def Train_model(training_data, training_labels,words,classes):\n",
        "    all_losses = []\n",
        "    learning_rate = 0.1\n",
        "    iterations = 50\n",
        "    np.random.seed(1)\n",
        "    X = training_data.T\n",
        "    print(\" Shape of X is \", X.shape)\n",
        "    Y = training_labels.T\n",
        "    print(\" Shape of Y is \", Y.shape)\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    print(\" Shape of m is \", m)\n",
        "    \n",
        "    n_h = 100\n",
        "    \n",
        "    n_x = X.shape[0]\n",
        "   \n",
        "    n_y = 4\n",
        "    \n",
        "    W1 = np.random.randn(n_h,n_x)*0.01\n",
        "    print(\" Shape of W1 is \", W1.shape)\n",
        "   \n",
        "    b1 = np.zeros((n_h,1))\n",
        "    \n",
        "    W2 = np.random.randn(n_y,n_h)\n",
        "    print(\" Shape of W2 is \", W2.shape)\n",
        "   \n",
        "    b2 = np.zeros((n_y,1))\n",
        "    print(\"################### TRAIN MODEL STATISTICS ######################\")\n",
        "    for i in range(0,iterations):\n",
        "        Z1,A1,Z2,A2 = forward_prop(n_x,n_h,n_y,m,X,W1,W2,b1,b2)\n",
        "        predictions = np.argmax(A2, axis=0)\n",
        "        labels = np.argmax(Y, axis=0)\n",
        "        print(classification_report(predictions,labels))\n",
        "        Loss = calculate_loss(Y,A2,m)\n",
        "        W1,b1,W2,b2 = back_prop(Z1,A1,Z2,A2,X,Y,W1,W2,b1,b2,learning_rate,m)\n",
        "        all_losses.append(Loss)\n",
        "\n",
        "    \n",
        "    weights = {'weight1': W1.tolist(), 'weight2': W2.tolist(),\n",
        "               'bias1':b1.tolist(), 'bias2':b2.tolist(),\n",
        "               'words': words,\n",
        "               'classes': classes\n",
        "              }\n",
        "    weights_file = \"weights.json\"\n",
        "\n",
        "    with open(weights_file, 'w') as outfile:\n",
        "        json.dump(weights, outfile, indent=4, sort_keys=True)\n",
        "    print (\"saved synapses to:\", weights_file)\n",
        "    plt.plot(all_losses)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_pcfYZDYkH98",
        "outputId": "8786a6ad-b574-49ab-af5b-614f2613c61e"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdHBefhyiQ8e",
        "outputId": "ea6da6d0-a155-4360-f3b1-9c336f0339c3"
      },
      "outputs": [],
      "source": [
        "\n",
        "ERROR_THRESHOLD = 0.1\n",
        "\n",
        "weights_file = 'weights.json'\n",
        "with open(weights_file) as data_file:\n",
        "    weights = json.load(data_file)\n",
        "    W1 = np.asarray(weights['weight1'])\n",
        "    W2 = np.asarray(weights['weight2'])\n",
        "    b1 = np.asarray(weights['bias1'])\n",
        "    b2 = np.asarray(weights['bias2'])\n",
        "    all_words = weights['words']\n",
        "    classes = weights['classes']\n",
        "\n",
        "def clean_sentence(verification_data):\n",
        "    line = verification_data\n",
        "   \n",
        "    line = line.strip().lower()\n",
        "    \n",
        "    line = \" \".join(filter(lambda x:x[0]!='@', line.split()))\n",
        "   \n",
        "    punct = line.maketrans(\"\",\"\",'.*%$^0123456789#!][\\?&/)/(+-<>')\n",
        "    result = line.translate(punct)\n",
        "    \n",
        "    tokened_sentence = nltk.word_tokenize(result)\n",
        "    \n",
        "    sentence = tokened_sentence[0:len(tokened_sentence)]\n",
        "    return sentence\n",
        "\n",
        "def verify(sentence, show_details=False):\n",
        "    bag=[0]*len(all_words)\n",
        "    cleaned_sentence = clean_sentence(sentence)\n",
        "   \n",
        "    x = encode_sentence(all_words,cleaned_sentence,bag)\n",
        "    x = np.array(x)\n",
        "    x = x.reshape(x.shape[0],1)\n",
        "\n",
        "    print(\"Shape of X is \", x.shape)\n",
        "    if show_details:\n",
        "        print (\"sentence:\", sentence, \"\\n bow:\", x)\n",
        "    \n",
        "    l0 = x\n",
        "   \n",
        "    l1 = relu(np.dot(W1,l0)+b1)\n",
        "    \n",
        "    l2 = softmax(np.dot(W2,l1)+b2)\n",
        "\n",
        "    return l2\n",
        "\n",
        "def classify(sentence, show_details=False):\n",
        "    results = verify(sentence, show_details)\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_results =[[classes[r[0]],r[1]] for r in results]\n",
        "    print (\"%s \\n classification: %s \\n\" % (sentence, return_results))\n",
        "    return return_results\n",
        "\n",
        "classify(\"I want to kill everyone @Name1 #why?\")\n",
        "classify(\"I am so happy @Name2 #yayyyy\")\n",
        "classify(\"This depression will kill me someday .. i am dying @Name3 #killme\")\n",
        "classify(\"I am afraid terrorists might attack us @Name4 #isis\")\n",
        "classify(\"What should I do when i am happy @Name5 \")\n",
        "classify(\"I want to be happy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLy9Rb7dp-5V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Attention\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, DecisionTreeRegressor, StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"text\"])\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df[\"text\"])\n",
        "\n",
        "\n",
        "max_seq_len = max(len(s) for s in sequences)\n",
        "sequences = pad_sequences(sequences, maxlen=max_seq_len)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, df[\"emotion\"], test_size=0.2)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index), output_dim=128))\n",
        "model.add(LSTM(128))\n",
        "model.add(Attention(attention_mechanism='bahdanau'))\n",
        "model.add(Dense(len(df[\"emotion\"].unique())))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "intensity_labels = {\n",
        "    0: \"Anger\",\n",
        "    1: \"Fear\",\n",
        "    2: \"sad\",\n",
        "    3: \"Joy\"\n",
        "}\n",
        "\n",
        "\n",
        "model = StackingRegressor([\n",
        "    ('rf', RandomForestRegressor()),\n",
        "    ('svr', SVR()),\n",
        "    ('ridge', Ridge()),\n",
        "    ('dt', DecisionTreeRegressor())\n",
        "])\n",
        "\n",
        "\n",
        "model.fit([y_pred, y_test], y_test)\n",
        "\n",
        "\n",
        "y_pred_intensity = model.predict([y_pred, y_test])\n",
        "\n",
        "\n",
        "print(\"Emotion classification accuracy:\", model.evaluate(X_test, y_test)[1])\n",
        "print(\"Emotion intensity prediction RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_intensity)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaH2xA6wq2sL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Attention\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, DecisionTreeRegressor, StackingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"text\"])\n",
        "\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(df[\"text\"])\n",
        "\n",
        "\n",
        "max_seq_len = max(len(s) for s in sequences)\n",
        "sequences = pad_sequences(sequences, maxlen=max_seq_len)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences, df[\"emotion\"], test_size=0.2)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index), output_dim=128,\n",
        "                   embeddings_initializer='distilbert', trainable=False))\n",
        "model.add(LSTM(128))\n",
        "model.add(Attention(attention_mechanism='bahdanau'))\n",
        "model.add(Dense(len(df[\"emotion\"].unique())))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "intensity_labels = {\n",
        "    0: \"anger\",\n",
        "    1: \"sad\",\n",
        "    2: \"fear\",\n",
        "    3: \"joy\"\n",
        "}\n",
        "\n",
        "\n",
        "model = StackingRegressor([\n",
        "    ('rf', RandomForestRegressor()),\n",
        "    ('svr', SVR()),\n",
        "    ('ridge', Ridge()),\n",
        "    ('dt', DecisionTreeRegressor())\n",
        "])\n",
        "\n",
        "\n",
        "combined_features = np.hstack((y_pred, y_test))\n",
        "\n",
        "\n",
        "model.fit(combined_features, y_test)\n",
        "\n",
        "\n",
        "y_pred_intensity = model.predict(combined_features)\n",
        "\n",
        "\n",
        "print(\"Emotion classification accuracy:\", model.evaluate(X_test, y_test)[1])\n",
        "print(\"Emotion intensity prediction RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_intensity)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kjvmmdFrdic"
      },
      "outputs": [],
      "source": [
        "\n",
        "emotion_data = pd.read_csv('emotion_data.csv')  \n",
        "X_text = emotion_data['text']\n",
        "y_emotion = emotion_data['emotion']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train_emotion, y_test_emotion = train_test_split(X_text, y_emotion, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "X_train_tokens = tokenizer.batch_encode_plus(X_train.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "X_test_tokens = tokenizer.batch_encode_plus(X_test.tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased', output_hidden_states=True)\n",
        "X_train_embeddings = model(**X_train_tokens)['hidden_states'][-2].mean(dim=1)\n",
        "X_test_embeddings = model(**X_test_tokens)['hidden_states'][-2].mean(dim=1)\n",
        "\n",
        "\n",
        "emotion_classifier = model()\n",
        "emotion_classifier.fit(X_train_embeddings.detach().numpy(), y_train_emotion)\n",
        "\n",
        "\n",
        "y_pred_emotion = emotion_classifier.predict(X_test_embeddings.detach().numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "intensity_data = pd.read_csv('intensity_data.csv')\n",
        "X_intensity = intensity_data['text']\n",
        "y_intensity = intensity_data['intensity']\n",
        "\n",
        "\n",
        "X_train_intensity, X_test_intensity, y_train_intensity, y_test_intensity = train_test_split(\n",
        "    X_intensity, y_intensity, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "X_train_combined = np.column_stack((X_train_embeddings.detach().numpy(), y_pred_emotion))\n",
        "X_test_combined = np.column_stack((X_test_embeddings.detach().numpy(), y_pred_emotion))\n",
        "\n",
        "\n",
        "ensemble_models = [\n",
        "    DecisionTreeRegressor(),\n",
        "    SVR(),\n",
        "    RandomForestRegressor(),\n",
        "    Ridge()\n",
        "]\n",
        "\n",
        "\n",
        "ensemble_predictions_train = []\n",
        "ensemble_predictions_test = []\n",
        "for model in ensemble_models:\n",
        "    model.fit(X_train_combined, y_train_intensity)\n",
        "    ensemble_predictions_train.append(model.predict(X_train_combined))\n",
        "    ensemble_predictions_test.append(model.predict(X_test_combined))\n",
        "\n",
        "X_train_stacked = np.column_stack(ensemble_predictions_train)\n",
        "X_test_stacked = np.column_stack(ensemble_predictions_test)\n",
        "\n",
        "\n",
        "final_model = LinearRegression()\n",
        "final_model.fit(X_train_stacked, y_train_intensity)\n",
        "\n",
        "\n",
        "y_pred_intensity = final_model.predict(X_test_stacked)\n",
        "\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test_intensity, y_pred_intensity))\n",
        "print(\"RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEHezzvqQuBy"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "input_sentiment = input(\"Hi :) How are you feeling today ? \")\n",
        "print(input_sentiment)\n",
        "print(classify(input_sentiment)[0][0])\n",
        "sentiment = classify(input_sentiment)[0][0]\n",
        "print(sentiment)\n",
        "if sentiment == \"anger\" or sentiment == \"sadness\" or sentiment == \"fear\":\n",
        "    answer = input(\"Sorry to hear that .... would you like to hear a joke to lighten your mood ? Press Yes or No \")\n",
        "    if answer == \"N\" or answer == \"No\" or answer == \"no\" or answer == \"n\":\n",
        "        print(\"Have a nice day. Goodbye :) \")\n",
        "    else:\n",
        "        file = open('content/jokes.txt','r')\n",
        "        while(1):\n",
        "            full_file = file.readline()\n",
        "            split_file = full_file.split('/')\n",
        "            print(split_file)\n",
        "            slashes = full_file.count('/')\n",
        "            print(slashes)\n",
        "            line_of_joke = []\n",
        "            for i in range(slashes):\n",
        "                k=0\n",
        "                print(split_file[i])\n",
        "                commas = split_file[i].count('\"')\n",
        "                print(commas)\n",
        "                length = int(commas/2)\n",
        "                if length == 0:\n",
        "                    line_of_joke.append(split_file[i])\n",
        "                else:\n",
        "                    for j in range(length):\n",
        "        \n",
        "                        line_of_joke.append(split_file[i].split('\"')[k]+split_file[i].split('\"')[k+1])\n",
        "                        if j==length-1:\n",
        "                            line_of_joke.append(split_file[i].split('\"')[k+2])\n",
        "                        k=k+2\n",
        "  \n",
        "            for i in line_of_joke:\n",
        "                print(i)\n",
        "            user_input = input(\"Do you want another joke ? Write Yes or No\\t\")\n",
        "            if user_input == \"Y\":\n",
        "                clear_output()\n",
        "                pass\n",
        "            else:\n",
        "                clear_output()\n",
        "                break\n",
        "        print(line_of_joke[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2RSkxyLSCAb",
        "outputId": "92836539-a1a0-4986-f72e-edf74949b025"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "input_sentiment = input(\"today is a good day\")\n",
        "print(input_sentiment)\n",
        "print(classify(input_sentiment)[0][0])\n",
        "sentiment = classify(input_sentiment)[0][0]\n",
        "print(sentiment)\n",
        "line=[]\n",
        "        file = open('content/model.txt','r')\n",
        "        while(1):\n",
        "            full_file = file.readline()\n",
        "            split_file = full_file.split('/')\n",
        "            print(split_file)\n",
        "            slashes = full_file.count('/')\n",
        "            print(slashes)\n",
        "\n",
        "            for i in range(slashes):\n",
        "                k=0\n",
        "               print(split_file[i])\n",
        "                commas = split_file[i].count('\"')\n",
        "               print(commas)\n",
        "                length = int(commas/2)\n",
        "                if length == 0:\n",
        "                    line.append(split_file[i])\n",
        "                else:\n",
        "                    for j in range(length):\n",
        "                    print(\"Here\")\n",
        "                        line.append(split_file[i].split('\"')[k]+split_file[i].split('\"')[k+1])\n",
        "                        if j==length-1:\n",
        "                            line.append(split_file[i].split('\"')[k+2])\n",
        "                        k=k+2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
